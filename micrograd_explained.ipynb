{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a python notebook made to explain the concept of neural networks and how you can code one yourself while using Mr Andrej Karpathy's amazing (<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0\">video</a>) as reference for all the code.\n",
    "\n",
    "<p style=\"color: red\">NOTE: THIS NOTEBOOK IS <b>NOT</b> MEANT TO REPLACE WATCHING THE <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0\">VIDEO</a>. ITS MEANT TO BE USED AS A GUIDE ALONGSIDE THE VIDEO AND FOR REVISION PURPOSES. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network ?\n",
    "To put it in simple terms, neural networks are mathematical expressions that take data as inputs (along with their corresponding weights and bias).  \n",
    "This data is then send through multiple 'layers' with each layer containing multiple 'neurons'. The network learns by adjusting the weights and biases of each neuron in order to <b>minimize the loss</b>. The lower the loss, the higher the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neuron ?\n",
    "It is the basic building block of a neural network. It takes in the following inputs: \n",
    "\n",
    "- Weights: This represents the importance level of the corresponding input. (how important a given feature is)\n",
    "- Bias: Addition of bias term helps in adjusting neuron's output.\n",
    "- Activation Function: This function helps to supress the output value within a specific range. It determines the neuron's outputs based on its inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Value object\n",
    "As we talked about in the prev paragraph, a Neuron takes in multiple values as input. Each value has some mathematical operation performed on it. This includes addition, multiplication etc.\n",
    "\n",
    "Below is a very diagram which helps in visualizing the concept of Neuron and Value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/simple_neuron.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure\n",
    "So we saw what a Neuron is and what a Value object is. \n",
    "Now lets talk a bit about what the overall structure of a neural network is. A simple neural network consists of 3 types of layers:  \n",
    "1. Input Layer: The number of neurons in this layer is based on the number of features/inputs from initial data \n",
    "2. Hidden Layer(s): These are one or more layers of neurons that process the information from the input layer. \n",
    "3. Output Layer: This layer produces the final results. Usually the number of (output) neurons in this layer depends on the problem beeing solved. Example, if we are training the neural network to classify 3 different types of dogs and so each output neuron will contain a probability of the input belonging to one of the three classes/types. \n",
    "\n",
    "We will now move on to talking about the different processes within a neural network starting with the forward pass.\n",
    "\n",
    "### Forward Pass\n",
    "The forward pass (also called the forward propagation) processes information in the following order:\n",
    "1. Each neuron receives inputs\n",
    "2. The neuron computes the weighted sum of the inputs. The equation to carry out the computation is : $$z = \\sum_{i=1}^n (w_i \\cdot x_i) + b$$\n",
    "where $ x_i $ is the input 'i' to the neuron and $ w_i $ is the weight of the corresponding neuron  \n",
    "3. After computing $ z $ we add a bias value to it and pass it through an activation function. To learn more about activation functions you can visit the following resources: \n",
    "    - <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\"> Link 1 </a>\n",
    "    - <a href=\"https://www.datacamp.com/tutorial/introduction-to-activation-functions-in-neural-networks\"> Link 2 </a>\n",
    "    - <a href=\"https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\"> Link 3 </a>\n",
    "\n",
    "4. The output from step 3 is then passed onto the next neuron as that neuron's input and the process then keeps following step 2 and step 3 until the output/final layer which produces the prediction.\n",
    "\n",
    "To give a further understanding lets take a look at a simple binary classification example which uses a neural network. \n",
    "\n",
    "<!-- insert neural network image with forward prop -->\n",
    "\n",
    "### Loss Calculation\n",
    "The next step would be to calculate the loss of the current neural network. 'Loss' refers to the difference between actual value (ground truth value) and the predicted value. \n",
    "This is an extremely crucial step as our main goal is to <b>minimize the loss</b> in order to achieve the best reuslts. \n",
    "The loss function we choose depends on the problem we are trying to model our neural network on.  \n",
    "For instance, if we have a regression problem we can use Cross-Entropy Loss. To understand which loss functions are use for which problems, check out this <a href=\"https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\">link</a>\n",
    "\n",
    "### Loss Gradient Calculaton\n",
    "Before looking at loss gradient, lets take a look at what 'gradient' is. Gradient can be thought of as the measure of how much a value changes when its corresponding input changes. A 'loss gradient' measures how the loss changes when we change the values of parameters (primarily weights). In neural networks we are concerned with obtaining the loss gradient with respect to the weights and biases.  \n",
    "By understanding how the loss behaves with changes to the weights and biases, you can adjust the weights and biases accordingly to minimize the loss.  \n",
    "To obtain the loss gradient we go through a process known as <b>backpropagation</b>\n",
    "\n",
    "### Backpropagation\n",
    "To summarize backpropagation, its basically the neural network learning from its mistakes.  \n",
    "The way backpropagation works is by obtaining the value of loss gradient with respect to all the parameters in the neural network (by parameters, I am referring to all the weights and biases in the model). We then adjust the values of the parameters according to the loss gradient wrt each of them. The reason its called 'back'propagation is because we start from the final/output layer and move through the hidden layer/s to the input layer. \n",
    "\n",
    "#### Mathematical Side of Backprop (tbh its literally all calculus)\n",
    "<p style=\"color: red\">Before proceeding with this part I would advise you to refresh yourself on the concepts of linear algebra and calculus. For those new to these concepts I would highly advise you to check out the <a href=\"https://www.khanacademy.org/math/calculus-1\">khan academy</a> course and the <a href=\"https://www.3blue1brown.com/topics/calculus\">3Blue1Brown</a> course to learn these concepts before moving further into this section</p>\n",
    "\n",
    "The entire process of backpropagation relies heavily on the calculus concept of chain rule. Here I will just write down the chain rule so that you can refer to it when going through the rest of the notebook.  \n",
    "For example, to calculate the loss gradient wrt weight 1 (w1) (To give you a better idea on how this works there is an accompanying neural network visualization for this example (generated thanks to claude 3.5) ):   \n",
    "<img src=\"./images/single_derivation.png\" width=\"600px\"/>  \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_1}$$  \n",
    "\n",
    "where: \n",
    "- w1 is the weight 1 (from x1 to z)\n",
    "- z is the weighted sum $$z = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + b$$\n",
    "- a is the activation value $$a = \\sigma(z) $$  (in this case sigmoid is the activation function)\n",
    "- L is the loss function\n",
    "\n",
    "\n",
    "NOTE: I will be working on a bigger example which will cover both forward prop and backprop full flow (meaning calculations for every parameter in the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding out micrograd\n",
    "\n",
    "micrograd is the implementation of the backpropagation step in neural networks. However as discussed earlier, to get to the backprop we need to start by implementing all the concepts that lead up to that point. The order we will follow is : \n",
    "- Creating the Value class\n",
    "- Creting the Neuron class\n",
    "- Creating the Layer class\n",
    "- Creating the MLP class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value class\n",
    "As defined wayyyyy earlier (kindly scroll up to check it out), a value object is basically value on which operations such as additon, subtraction etc are carried out. \n",
    "You will also notice the _backward() function in each of the operation classes. This function plays a key part in the backprop process. For each operation:   \n",
    "1. Addition: Distributing the gradient to both inputs  \n",
    "\n",
    "2. Multiplication: Here we update the gradients for both inputs based on the chain rule   \n",
    "\n",
    "3. Activation function: Each activation function has its own derivative (obviously). In the case of micrograd we use the tanh activation function and the _backward() function returns the derivative accordingly  \n",
    "$$\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)$$  \n",
    "4. Power operation: Computes gradient for power op according to the rule: $$\\frac{d}{dx} x^n = nx^{n-1} $$\n",
    "\n",
    "<p style=\"color: red\">Do not skip any parts of the video especially not the main coding parts. Here I am giving brief overview of each of the classes but its implementation is best explained in the video</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# creating the Value object and enabling functions such as addition and subtraction\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\") -> None:\n",
    "        \"\"\"\n",
    "        data: value of the neuron (activation value)\n",
    "        _children: set. used to store the children of the node\n",
    "        label: neuron reference name\n",
    "        _op: operation performed to produce the Value.\n",
    "        label: label on the Value object\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        # keeping track of derivative of the node\n",
    "        self.grad = 0\n",
    "        # store chain rule. by default does nothing\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        print the value stored within the Value object\n",
    "        \"\"\"\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other) -> int | float:\n",
    "        # if we are trying to add integer to a Value object, convert the number into a Value object then extract its value and add\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            # here we accumulate gradient as it is possible we may have multivariable contribution backwards. depositing gradients from mutliple branches\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other) -> int | float:\n",
    "        \"\"\"\n",
    "        other: usually this is the weight\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # this is basically a fallback for if the __mul__ function does not get the inputs in the other self, other. to learn more about it try this link : https://stackoverflow.com/questions/5181320/under-what-circumstances-are-rmul-called\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # self * 1/other = self/other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # subtraction implementation\n",
    "        return self + (-other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        # power function.\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            # multiplying out.grad is essnetial to the chain rule\n",
    "            new_val = other - 1\n",
    "            self.grad += other * (self.data**new_val) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        This is the main activation function used \n",
    "        \"\"\"\n",
    "        x = self.data\n",
    "        t = (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            # derivative of the tanh func. starting from loss L\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Exponent\n",
    "        \"\"\"\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Extremely important function that helps us traverse through each node in reverse order starting from the final layer. \n",
    "        \"\"\"\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Class\n",
    "This is a straightforward implementation of neuron where we initialize each Neuron with a list of weights (its length matching the number of input neurons from previous layer) and a bias value\n",
    "\n",
    "We use the equation $$z = \\sigma(\\sum_{i=1}^n (w_i \\cdot x_i) + b)$$ to calculate the activation value of the Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron class\n",
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    # neuron class within which we will define incoming inputs/weights as well as its bias\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        \"\"\"\n",
    "        nin: Number of inputs. Equivalent to number of neurons in input layer\n",
    "        \"\"\"\n",
    "        # initializing collection of weights -> number of inputs/inital neurons\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calls the equation to calculate activation value: (w * x) + b\n",
    "        # if you are unfamilar with zip() check this out: https://www.geeksforgeeks.org/zip-in-python/\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # apply activation function for non-linearity + suppress value within a specified range\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        returns the weight list and the bias value for the neuron\n",
    "        \"\"\"\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # a layer contains multiple Neuron objects\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"\n",
    "        nin:  number of incoming neurons/weights for one Neuron in layer\n",
    "        nout: number of outputs/how many neurons do you want in the layer\n",
    "        \"\"\"\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\" \"\"\"\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            for p in neuron.parameters():\n",
    "                params.append(p)\n",
    "        return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

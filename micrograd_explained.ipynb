{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a python notebppk made to explain the concept of neural networks and how you can code one yourself while using Mr Andrej Karpathy's amazing (<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0\">video</a>) as reference for all the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network ?\n",
    "To put it in simple terms, neural networks are mathematical expressions that take data as inputs (along with their corresponding weights and bias).  \n",
    "This data is then send through multiple 'layers' with each layer containing multiple 'neurons'. The network learns by adjusting the weights and biases of each neuron in order to <b>minimize the loss</b>. The lower the loss, the higher the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neuron ?\n",
    "It is the basic building block of a neural network. It takes in the following inputs: \n",
    "\n",
    "- Weights: This represents the importance level of the corresponding input. (how important a given feature is)\n",
    "- Bias: Addition of bias term helps in adjusting neuron's output.\n",
    "- Activation Function: This function helps to supress the output value within a specific range. It determines the neuron's outputs based on its inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Value object\n",
    "As we talked about in the prev paragraph, a Neuron takes in multiple values as input. Each value has some mathematical operation performed on it. This includes addition, multiplication etc.\n",
    "\n",
    "Below is a very diagram which helps in visualizing the concept of Neuron and Value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/simple_neuron.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure\n",
    "So we saw what a Neuron is and what a Value object is. \n",
    "Now lets talk a bit about what the overall structure of a neural network is. A simple neural network consists of 3 types of layers:  \n",
    "1. Input Layer: The number of neurons in this layer is based on the number of features/inputs from initial data \n",
    "2. Hidden Layer(s): These are one or more layers of neurons that process the information from the input layer. \n",
    "3. Output Layer: This layer produces the final results. Usually the number of (output) neurons in this layer depends on the problem beeing solved. Example, if we are training the neural network to classify 3 different types of dogs and so each output neuron will contain a probability of the input belonging to one of the three classes/types. \n",
    "\n",
    "We will now move on to talking about the different processes within a neural network starting with the forward pass.\n",
    "\n",
    "### Forward Pass\n",
    "The forward pass (also called the forward propagation) processes information in the following order:\n",
    "1. Each neuron receives inputs\n",
    "2. The neuron computes the weighted sum of the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# creating the Value object and enabling functions such as addition and subtraction\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\") -> None:\n",
    "        \"\"\"\n",
    "        data: value of the neuron (activation value)\n",
    "        _children: set. used to store the children of the node\n",
    "        label: neuron reference name\n",
    "        _op: operation performed to produce the Value. \n",
    "        label: label on the Value object\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        # keeping track of derivative of the node\n",
    "        self.grad = 0\n",
    "        # store chain rule. by default does nothing\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        print the value stored within the Value object\n",
    "        \"\"\"\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other) -> int | float:\n",
    "        # if we are trying to add integer to a Value object, convert the number into a Value object then extract its value and add\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            # here we accumulate gradient as it is possible we may have multivariable contribution backwards. depositing gradients from mutliple branches\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other) -> int | float:\n",
    "        \"\"\"\n",
    "        other: usually this is the weight\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # this is basically a fallback for if the __mul__ function does not get the inputs in the other self, other. to learn more about it try this link : https://stackoverflow.com/questions/5181320/under-what-circumstances-are-rmul-called\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # self * 1/other = self/other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # subtraction implementation\n",
    "        return self + (-other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        # power function.\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            # multiplying out.grad is essnetial to the chain rule\n",
    "            new_val = other - 1\n",
    "            self.grad += other * (self.data**new_val) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            # derivative of the tanh func. starting from loss L\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

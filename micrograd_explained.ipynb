{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a python notebppk made to explain the concept of neural networks and how you can code one yourself while using Mr Andrej Karpathy's amazing (<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0\">video</a>) as reference for all the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network ?\n",
    "To put it in simple terms, neural networks are mathematical expressions that take data as inputs (along with their corresponding weights and bias).  \n",
    "This data is then send through multiple 'layers' with each layer containing multiple 'neurons'. The network learns by adjusting the weights and biases of each neuron in order to <b>minimize the loss</b>. The lower the loss, the higher the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neuron ?\n",
    "It is the basic building block of a neural network. It takes in the following inputs: \n",
    "\n",
    "- Weights: This represents the importance level of the corresponding input. (how important a given feature is)\n",
    "- Bias: Addition of bias term helps in adjusting neuron's output.\n",
    "- Activation Function: This function helps to supress the output value within a specific range. It determines the neuron's outputs based on its inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Value object\n",
    "As we talked about in the prev paragraph, a Neuron takes in multiple values as input. Each value has some mathematical operation performed on it. This includes addition, multiplication etc.\n",
    "\n",
    "Below is a very diagram which helps in visualizing the concept of Neuron and Value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/simple_neuron.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure\n",
    "So we saw what a Neuron is and what a Value object is. \n",
    "Now lets talk a bit about what the overall structure of a neural network is. A simple neural network consists of 3 types of layers:  \n",
    "1. Input Layer: The number of neurons in this layer is based on the number of features/inputs from initial data \n",
    "2. Hidden Layer(s): These are one or more layers of neurons that process the information from the input layer. \n",
    "3. Output Layer: This layer produces the final results. Usually the number of (output) neurons in this layer depends on the problem beeing solved. Example, if we are training the neural network to classify 3 different types of dogs and so each output neuron will contain a probability of the input belonging to one of the three classes/types. \n",
    "\n",
    "We will now move on to talking about the different processes within a neural network starting with the forward pass.\n",
    "\n",
    "### Forward Pass\n",
    "The forward pass (also called the forward propagation) processes information in the following order:\n",
    "1. Each neuron receives inputs\n",
    "2. The neuron computes the weighted sum of the inputs. The equation to carry out the computation is : $$z = \\sum_{i=1}^n (w_i \\cdot x_i) + b$$\n",
    "where $ x_i $ is the input 'i' to the neuron and $ w_i $ is the weight of the corresponding neuron  \n",
    "3. After computing $ z $ we add a bias value to it and pass it through an activation function. To learn more about activation functions you can visit the following resources: \n",
    "    - <a href=\"https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\"> Link 1 </a>\n",
    "    - <a href=\"https://www.datacamp.com/tutorial/introduction-to-activation-functions-in-neural-networks\"> Link 2 </a>\n",
    "    - <a href=\"https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\"> Link 3 </a>\n",
    "\n",
    "4. The output from step 3 is then passed onto the next neuron as that neuron's input and the process then keeps following step 2 and step 3 until the output/final layer which produces the prediction.\n",
    "\n",
    "To give a further understanding lets take a look at a simple binary classification example which uses a neural network. \n",
    "\n",
    "<!-- insert neural network image with forward prop -->\n",
    "\n",
    "### Loss Calculation\n",
    "The next step would be to calculate the loss of the current neural network. 'Loss' refers to the difference between actual value (ground truth value) and the predicted value. \n",
    "This is an extremely crucial step as our main goal is to <b>minimize the loss</b> in order to achieve the best reuslts. \n",
    "The loss function we choose depends on the problem we are trying to model our neural network on.  \n",
    "For instance, if we have a regression problem we can use Cross-Entropy Loss. To understand which loss functions are use for which problems, check out this <a href=\"https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\">link</a>\n",
    "\n",
    "### Loss Gradient Calculaton\n",
    "Before looking at loss gradient, lets take a look at what 'gradient' is. Gradient can be thought of as the measure of how much a value changes when its corresponding input changes. A 'loss gradient' measures how the loss changes when we change the values of parameters (primarily weights). In neural networks we are concerned with obtaining the loss gradient with respect to the weights and biases.  \n",
    "By understanding how the loss behaves with changes to the weights and biases, you can adjust the weights and biases accordingly to minimize the loss.  \n",
    "To obtain the loss gradient we go through a process known as <b>backpropagation</b>\n",
    "\n",
    "### Backpropagation\n",
    "To summarize backpropagation, its basically the neural network learning from its mistakes.  \n",
    "The way backpropagation works is by obtaining the value of loss gradient with respect to all the parameters in the neural network (by parameters, I am referring to all the weights and biases in the model). We then adjust the values of the parameters according to the loss gradient wrt each of them. The reason its called 'back'propagation is because we start from the final/output layer and move through the hidden layer/s to the input layer. \n",
    "\n",
    "#### Mathematical Side of Backprop (tbh its literally all calculus)\n",
    "<p style=\"color: red\">Before proceeding with this part I would advise you to refresh yourself on the concepts of linear algebra and calculus. For those new to these concepts I would highly advise you to check out the <a href=\"https://www.khanacademy.org/math/calculus-1\">khan academy</a> course and the <a href=\"https://www.3blue1brown.com/topics/calculus\">3Blue1Brown</a> course to learn these concepts before moving further into this section</p>\n",
    "\n",
    "The entire process of backpropagation relies heavily on the calculus concept of chain rule. Here I will just write down the chain rule so that you can refer to it when going through the rest of the notebook.  \n",
    "For example, to calculate the loss gradient wrt weight 1 (w1) (To give you a better idea on how this works there is an accompanying neural network visualization for this example (generated thanks to claude 3.5) ):   \n",
    "<img src=\"./images/single_derivation.png\" width=\"600px\"/>  \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_1}$$  \n",
    "\n",
    "where: \n",
    "- w1 is the weight 1 (from x1 to z)\n",
    "- z is the weighted sum $$z = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + b$$\n",
    "- a is the activation value $$a = \\sigma(z) $$  (in this case sigmoid is the activation function)\n",
    "- L is the loss function\n",
    "\n",
    "\n",
    "#### Working through a complete backprop example for every parameter\n",
    "In this subsection, i will work through an entire example for backpropagation (apologies in advance for the rough handwriting :(    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# creating the Value object and enabling functions such as addition and subtraction\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\") -> None:\n",
    "        \"\"\"\n",
    "        data: value of the neuron (activation value)\n",
    "        _children: set. used to store the children of the node\n",
    "        label: neuron reference name\n",
    "        _op: operation performed to produce the Value. \n",
    "        label: label on the Value object\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        # keeping track of derivative of the node\n",
    "        self.grad = 0\n",
    "        # store chain rule. by default does nothing\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        print the value stored within the Value object\n",
    "        \"\"\"\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other) -> int | float:\n",
    "        # if we are trying to add integer to a Value object, convert the number into a Value object then extract its value and add\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            # here we accumulate gradient as it is possible we may have multivariable contribution backwards. depositing gradients from mutliple branches\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other) -> int | float:\n",
    "        \"\"\"\n",
    "        other: usually this is the weight\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # this is basically a fallback for if the __mul__ function does not get the inputs in the other self, other. to learn more about it try this link : https://stackoverflow.com/questions/5181320/under-what-circumstances-are-rmul-called\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # self * 1/other = self/other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # subtraction implementation\n",
    "        return self + (-other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        # power function.\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            # multiplying out.grad is essnetial to the chain rule\n",
    "            new_val = other - 1\n",
    "            self.grad += other * (self.data**new_val) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            # derivative of the tanh func. starting from loss L\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
